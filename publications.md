---
layout: default
---

# Publications

<div class="pub-card">
<div class="pub-card-text">
<div class="pub-card-title">Transformation of acoustic information to sensory decision variables in the parietal cortex. </div> 
<div class="pub-card-subtitle"> Justin D. Yao, Klavdia O. Zemlianovaa, David L. Hockera, Cristina Savina, Christine M. Constantinople, SueYeon Chung,  and Dan H. Sanes. </div> 
<div class="pub-card-body"> The process by which sensory evidence contributes to perceptual choices requires an understanding of its transformation into decision variables. Here, we address this issue by evaluating the neural representation of acoustic information in the auditory cortex- recipient parietal cortex, while gerbils either performed a two-alternative forced-choice auditory discrimination task or while they passively listened to identical acoustic stimuli. Our findings demonstrate how parietal cortex neurons integrate and transform encoded auditory information to guide sound-driven perceptual decisions[<a id="external-link" href="https://www.pnas.org/doi/epdf/10.1073/pnas.2212120120">pdf</a>] [<a id="external-link" href="assets/bib/yao2022.html">bib</a>]</div>
</div>
<img class="pub-card-img" src="{{site.baseurl | prepend:site.url}}assets/img/yao-et-al.png"/>
</div>

<hr>
<div class="pub-card">
<div class="pub-card-text">
<div class="pub-card-title"> The Implicit Bias of Gradient Descent on
Generalized Gated Linear Networks </div> 
<div class="pub-card-subtitle"> Samuel Lippl, L.F. Abbott, and SueYeon Chung, </div> 
<div class="pub-card-body"> Understanding the asymptotic behavior of gradient-descent training of deep neural networks is essential for revealing inductive biases and improving network performance. We derive the infinite-time training limit of a mathematically tractable class of deep nonlinear neural networks, gated linear networks (GLNs), and generalize these results to gated networks described by general homogeneous polynomials.  [<a id="external-link" href="https://arxiv.org/pdf/2202.02649.pdf">pdf</a>] [<a id="external-link" href="assets/bib/lippl2022.html">bib</a>]</div>
</div>
<img class="pub-card-img" src="{{site.baseurl | prepend:site.url}}assets/img/lippl-et-al.png"/>
</div>

<hr>
<div class="pub-card">
<div class="pub-card-text">
<div class="pub-card-title"> Divisive Feature Normalization Improves Image Recognition Performance in AlexNet</div> 
<div class="pub-card-subtitle"> Michelle Miller, SueYeon Chung, Kenneth D. Miller </div> 
<div class="pub-card-body"> Local divisive normalization provides a phenomenological description of many nonlinear response properties of neurons across visual cortical areas. To gain insight into the utility of this operation, we studied the effects on AlexNet of a local divisive normalization between features, with learned parameters.   [<a id="external-link" href="https://openreview.net/pdf?id=aOX3a9q3RVV">pdf</a>] [<a id="external-link" href="assets/bib/miller2022.html">bib</a>]</div>
</div>
<img class="pub-card-img" src="{{site.baseurl | prepend:site.url}}assets/img/miller-et-al.png"/>
</div>

<hr>
<div class="pub-card">
<div class="pub-card-text">
<div class="pub-card-title"> Neural population geometry: An approach for understanding biological and artificial neural networks</div> 
<div class="pub-card-subtitle"> SueYeon Chung, Larry Abbott </div> 
<div class="pub-card-body"> We highlight recent studies of neural population geometry: untangling in perception, classification theory of manifolds, abstraction in cognitive systems, topology underlying cognitive maps, dynamic untangling in motor systems, and a dynamic approach to cognition.   [<a id="external-link" href="https://reader.elsevier.com/reader/sd/pii/S0959438821001227?token=A35A4C64517A0C79DEC5D1818739BAABEB465C461F9E49F7DED819CB2B8B6A37596F353C4C04FA8C561E9CD86D2C1B2D&originRegion=us-east-1&originCreation=20230118192945">pdf</a>] [<a id="external-link" href="assets/bib/chung-and-abbott.html">bib</a>]</div>
</div>
<img class="pub-card-img" src="{{site.baseurl | prepend:site.url}}assets/img/chung-and-abbott.png"/>
</div>

<hr>
<div class="pub-card">
<div class="pub-card-text">
<div class="pub-card-title"> Credit Assignment Through Broadcasting a Global Error Vector </div> 
<div class="pub-card-subtitle"> David G. Clark, L.F. Abbott, SueYeon Chung</div> 
<div class="pub-card-body"> Here, we explore the extent to which a globally broadcast learning signal, coupled with local weight updates, enables training of DNNs. We present both a learning rule, called global error-vector broadcasting (GEVB), and a class of DNNs, called vectorized nonnegative networks (VNNs), inwhich this learning rule operates.  [<a id="external-link" href="https://arxiv.org/pdf/2106.04089.pdf">pdf</a>] [<a id="external-link" href = "https://github.com/davidclark1/VectorizedNets">code</a>] [<a id="external-link" href="assets/bib/clark2021.html">bib</a>]</div>
</div>
<img class="pub-card-img" src="{{site.baseurl | prepend:site.url}}assets/img/clark-et-al.png"/>
</div>

<hr>
<div class="pub-card">
<div class="pub-card-text">
<div class="pub-card-title"> Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models </div> 
<div class="pub-card-subtitle"> Matteo Alleman, Jonathan Mamou, Miguel A Del Rio, Hanlin Tang, Yoon Kim+, SueYeon Chung+ </div> 
<div class="pub-card-body"> It is not entirely clear what aspects of sentence-level syntax are captured by  vector-based language representations, nor how (if at all) they are built along the stacked layers of the network. In this paper, we aim to address such questions with a general class of interventional, input perturbation-based analyses of representations from pretrained language models. Importing from computational and cognitive neuroscience the notion of representational invariance, we perform a series of probes designed to test the sensitivity of these representations to several kinds of structure in sentences.  [<a id="external-link" href="https://aclanthology.org/2021.repl4nlp-1.27.pdf">pdf</a>] [<a id="external-link" href="assets/bib/alleman-et-al.html">bib</a>]</div>
</div>
<img class="pub-card-img" src="{{site.baseurl | prepend:site.url}}assets/img/alleman-et-al.png"/>
</div>

<hr>
<div class="pub-card">
<div class="pub-card-text">
<div class="pub-card-title"> Understanding the Logit Distributions of
Adversarially-Trained Deep Neural Networks</div> 
<div class="pub-card-subtitle"> Landan Seguin, Anthony Ndirango, Neeli Mishra, SueYeon Chung, Tyler Lee </div> 
<div class="pub-card-body"> Although adversarial training is successful at mitigating adversarial attacks, the behavioral differences between adversarially-trained (AT) models and standard models are still poorly understood. Motivated by a recent study on learning robustness without input perturbations by distilling an AT model, we explore what is learned during adversarial training by analyzing the distribution of logits in AT models.   [<a id="external-link" href="https://arxiv.org/pdf/2108.12001.pdf">pdf</a>] [<a id="external-link" href="assets/bib/seguin-et-al.html">bib</a>]</div>
</div>
<img class="pub-card-img" src="{{site.baseurl | prepend:site.url}}assets/img/seguin-et-al.png"/>
</div>

